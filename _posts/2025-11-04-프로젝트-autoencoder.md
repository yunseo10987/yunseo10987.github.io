---
title: "[2025-11-04] AutoEncoder"
excerpt: "ì„¤ëª…ì¶© í”„ë¡œì íŠ¸"

categories:
  - Project
tags:
  - [NetWork]

permalink: /Project/[2025-11-04] AutoEncoder/

toc: true
toc_sticky: true

date: 2025-11-04
last_modified_at: 2025-11-04
---

## ğŸ¦¥ ë³¸ë¬¸

### ì •ì˜

Unlabeled dataì˜ íš¨ê³¼ì ì¸ ì½”ë”©ì„ ë°°ìš°ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ì¸ê³µ ì‹ ê²½ë§ì˜ í•œ í˜•íƒœ. ì…ë ¥ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•˜ê³  ë‹¤ì‹œ ë””ì½”ë”©í•˜ë„ë¡ í›ˆë ¨ë˜ëŠ” íŠ¹ìˆ˜í•œ ì¢…ë¥˜ì˜ ì‹ ê²½ë§. 

### êµ¬ì¡°

![image.png](https://yunseo10987.github.io/assets/images/posts_img/2025-11-04%20autoencoder/image.png)

Encoder : ì…ë ¥ ë°ì´í„°ë¥¼ meaningfulí•˜ê³  compressedëœ represntationìœ¼ë¡œ ì¸ì½”ë”©. ë°ì´í„°ë¥¼ ìš”ì•½í•˜ê³  ì••ì¶•í•˜ì—¬ ì ì¬ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê°€ì¥ í•µì‹¬ì ì¸ íŠ¹ì§•ë§Œ ë‚¨ê¸°ë„ë¡ í›ˆë ¨  

Decoder : Encoding ëœ representationì„ ë‹¤ì‹œ ì…ë ¥ ë°ì´í„°ë¡œ ë³µì› 

- ì ì¬ ê³µê°„ : ê³ ì°¨ì›ì˜ ì›ë³¸ ë°ì´í„°ê°€ ì••ì¶•ë˜ì–´ ê·¸ í•µì‹¬ íŠ¹ì§•ê³¼ íŒ¨í„´ì´ ì €ì¥ë˜ëŠ” ì €ì°¨ì›ì˜ ê³µê°„

â†’ ì…ë ¥ ë°ì´í„°ë§Œì„ í™œìš©í•˜ëŠ” ë¹„ì§€ë„ í•™ìŠµìœ¼ë¡œ ì…ë ¥ ë°ì´í„°ì™€ ë³µì›ëœ ë°ì´í„°ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ

### ëª©ì 

1. ì…ë ¥ ë°ì´í„°ë¥¼ ì˜ë¯¸ ìˆê³  ì••ì¶•ëœ í‘œí˜„ìœ¼ë¡œ ë§¤í•‘í•´ì£¼ëŠ” ì¸ì½”ë”ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ëª©ì  â†’ manifold learningì„ ìœ„í•œ ì¸ì½”ë” í•™ìŠµ. ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•¨ 
    - ê³¼ì í•© ë¬¸ì œ : ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ë¥¼ ì§€ë‚˜ì¹˜ê²Œ í•™ìŠµí•˜ì—¬ ìƒˆë¡­ê³  ë³¸ ì ì´ ì—†ëŠ” ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§€ëŠ” í˜„ìƒ. í›ˆë ¨ ë°ì´í„°ì—ì„œëŠ” ì™„ë²½í•˜ì§€ë§Œ ì‹¤ì œ ë°ì´í„°ì—ì„œëŠ” ì“¸ëª¨ ì—†ì–´ì§
    - Sparse Autoencoder, Denoising Autoencoder, Contractive Autoencoder ë“±ì´ ìˆìŒ
2. ì ì¬ ê³µê°„ì„ ì‹¤ì œ data distributionìœ¼ë¡œ mapping í•´ì£¼ëŠ” ë””ì½”ë”ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ëª©ì  â†’ generative modelì„ ìœ„í•œ ë””ì½”ë” í•™ìŠµ 
    - Variational Autoencoderê°€ ìˆìŒ

### ì£¼ìš” ê°œë…

- Manifold : ë‘ ì ì‚¬ì´ì˜ ê±°ë¦¬ í˜¹ì€ ìœ ì‚¬ë„ê°€ ê·¼ê±°ë¦¬ì—ì„œëŠ” ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ë¥¼ ë”°ë¥´ì§€ë§Œ ì›ê±°ë¦¬ì—ì„œëŠ” ê·¸ëŸ¬ì§€ ì•ŠëŠ” ê³µê°„.
    
    êµ­ì†Œì ìœ¼ë¡œëŠ” ìš°ë¦¬ê°€ ì‚¬ëŠ” í‰í‰í•œ ê³µê°„ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œ íœ˜ì–´ì ¸ ìˆê±°ë‚˜ ë…íŠ¹í•œ êµ¬ì¡°ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê³µê°„. ê³ ì°¨ì› ê³µê°„ì— ë‚´ì¬í•œ ì €ì°¨ì› ê³µê°„. ë§ˆì¹˜ ì§€êµ¬.
    
    - manifold ê°€ì • : í˜„ì‹¤ì˜ ê³ ì°¨ì› ë°ì´í„°ëŠ” ì‹¤ì œë¡œëŠ” ê·¸ ê³µê°„ ì „ì²´ë¥¼ ì±„ìš°ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í›¨ì”¬ ë‚®ì€ ì°¨ì›ì˜ ìˆ¨ê²¨ì§„ êµ¬ì¡°ë¥¼ ë”°ë¼ ë¶„í¬í•˜ê³  ìˆë‹¤ëŠ” ê°€ì •. ì´ ìˆ¨ê²¨ì§„ êµ¬ì¡°ë¥¼ ë°ì´í„° ë§¤ë‹ˆí´ë“œë¼ê³  í•¨
        
        ê³ ì°¨ì› ê³µê°„ì— ì£¼ì–´ì§„ ì‹¤ì œ ì„¸ê³„ì˜ ë°ì´í„°ëŠ” ê³ ì°¨ì› ì…ë ¥ ê³µê°„ì— ë‚´ì¬í•œ í›¨ì”¬ ì €ì°¨ì› ë§¤ë‹ˆí´ë“œì˜ ì¸ê·¼ì— ì§‘ì¤‘ë˜ì–´ ìˆë‹¤.  
        
        - íƒœì–‘ê³„ì— ê´‘í™œí•œ ëª¨ë“  ìœ„ì¹˜ëŠ” ë°ì´í„°ê°€ ì¡´ì¬í•  ìˆ˜ ìˆëŠ” ê³µê°„ì„. ê·¼ë° ì§€êµ¬ë¼ëŠ” ë§¤ë‹ˆí´ë“œ ê³µê°„ì—ë§Œ ì˜ë¯¸ ìˆëŠ” ë°ì´í„°(ìƒë¬¼)ì´ ì‚´ê³  ìˆìŒ. ê·¸ë¦¬ê³  ê·¸ í•´ë‹¹ ê³µê°„ì„ êµ­ì†Œì ìœ¼ë¡œ ë³´ë©´ 2ì°¨ì› í‰ë©´ì²˜ëŸ¼ ë³´ì„. ê·¸ë¦¬ê³  í‘œë©´ì„ ë”°ë¼ ê±¸ìœ¼ë©´ë¶€ë“œëŸ½ê²Œ ë‹¤ë¥¸ ì˜ë¯¸ ìˆëŠ” ë°ì´í„°ë¡œ ì „í™˜ì´ ê°€ëŠ¥í•¨
- Manifold Learning : ê³ ì°¨ì› ê³µê°„ì˜ ë°ì´í„°ë¥¼ ì €ì°¨ì› manifold ê³µê°„ìœ¼ë¡œ mapping ì‹œí‚¤ëŠ” í•¨ìˆ˜ë¥¼ ì°¾ëŠ” ê³¼ì •. í•™ìŠµì´ ëë‚œ autoencoderì˜ encoderë¥¼ mapping í•¨ìˆ˜ë¡œ ì‚¬ìš©
- Generative Model : ì‹¤ì œ ë°ì´í„° ë¶„í¬ë¥¼ í•™ìŠµí•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ìƒì„± ëª¨ë¸. ì ì¬ ê³µê°„ì„ ì‹¤ì œ ë°ì´í„° ë¶„í¬ë¡œ ë§¤í•‘ì‹œí‚¤ëŠ” í•¨ìˆ˜ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì . í•™ìŠµì´ ëë‚œ Autoencoderì˜ decoderë¥¼ ë§¤í•‘ í•¨ìˆ˜ë¡œ ì‚¬ìš©
- ì‚¬ì „ í•™ìŠµ
    - Total Error = Bias^2  + Variance + Irreducible Error
        - Bias : ì‹¤ì œ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” ëª¨ë¸ê³¼ ê°€ì •í•œ ëª¨ë¸ì˜ ì°¨ì´ì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜. ì‹¤ì œê°’ê³¼ í‰ê·  ì˜ˆì¸¡ê°’ì˜ ì°¨ì´
        - Variance : ëª¨ë¸ë§ì— ì‚¬ìš©ë˜ëŠ” ì—¬ëŸ¬ í‘œë³¸ ë°ì´í„° ì§‘í•©ì— ëŒ€í•œ ì¶”ì •ì„ í•  ë•Œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜. ì˜ˆì¸¡ê°’ì— ëŒ€í•œ ë¶„ì‚°. 
        ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì…‹ì˜ ì‘ì€ ë³€í™”ë‚˜ ë…¸ì´ì¦ˆì— ë„ˆë¬´ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ì—¬, ìƒˆë¡œìš´ ë°ì´í„°ì…‹(ë˜ëŠ” ê²€ì¦ì…‹)ì´ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ì˜ˆì¸¡ê°’ì˜ **ë³€ë™(ë¶„ì‚°)**ì´ ë§¤ìš° í¬ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ì˜¤ë¥˜
        - Irreducible Error : ì¤„ì¼ ìˆ˜ ì—†ëŠ” ìì—°ì ì¸ ì˜¤ë¥˜
        
        Biasì™€ VarianceëŠ” trade-off ê´€ê³„. ì°¨ìˆ˜ê°€ ë†’ì•„ì§ˆ ìˆ˜ë¡(ëª¨ë¸ì´ ë³µì¡í•´ì§ˆ ìˆ˜ë¡) BiasëŠ” ì¤„ì–´ë“¤ì§€ë§Œ VarianceëŠ” ëŠ˜ì–´ë‚¨ 
        
        í›ˆë ¨ ì§‘í•©ê³¼ ê²€ì¦ ì§‘í•© ê´€ì ì—ì„œ í›ˆë ¨ ì§‘í•©ì— ëŒ€í•œ ì˜¤ë¥˜ìœ¨ì´ ë„ˆë¬´ ì‘ì•„ì§€ë©´ ê²€ì¦ ì§‘í•©ì— ëŒ€í•œ variabceê°€ ì»¤ì ¸ ê³¼ì í•© ë°œìƒ  
        
        â†’ ì í•©í•œ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì°¾ì•„ì•¼ í•¨. ë³µì¡ë„ë¥¼ ë†’ì¸ ë’¤, ë‹¤ì–‘í•œ ê·œì œ ê¸°ë²•ì„ í†µí•´ì„œ biasëŠ” ë†’ì´ê³  varianceë¥¼ ë‚®ì¶”ëŠ” ë°©í–¥ìœ¼ë¡œ ë°œì „ 
        
    - ê·œì œ : ëª¨ë¸ì— ì œì•½ì„ ì£¼ì–´ì„œ ëª¨ë¸ì´ ê°€ì§„ ëª¨ë“  ìš©ëŸ‰ì„ ì‚¬ìš©í•˜ì§€ ëª»í•˜ê²Œ í•¨ â†’ ì•”ê¸°ë¥¼ ì˜ˆë°©
- Bias-variance tradeoff in autoencoder
    
    ë°ì´í„°ë¥¼ ì˜ ë³µì›í•¨ê³¼ ë™ì‹œì—(low bias) ì²˜ìŒ ë³´ëŠ” ë°ì´í„°ë„ ì•„ìš°ë¥¼ ìˆ˜ ìˆëŠ” representationì„ í•™ìŠµ(low variance)í•˜ê¸¸ ë°”ëŒ â†’ ë³µì¡ë„ë¥¼ ë†’ì´ê³  ê·œì œ ê¸°ë²•ì„ ì ìš© 
    

### ì¢…ë¥˜ : for manifold learning

1. Sparse Autoencoder : ì€ë‹‰ì¸µ ë…¸ë“œì˜ ìˆ˜ê°€ ì…ë ¥ì¸µ ë…¸ë“œì˜ ìˆ˜ë³´ë‹¤ ë§ì€ overcomplete autoencoder êµ¬ì¡°. ì€ë‹‰ì¸µì—ì„œ í™œì„±í™”ë˜ëŠ” ë…¸ë“œê°€ ì ì–´ì§ 
    
    ![image.png](https://yunseo10987.github.io/assets/images/posts_img/2025-11-04%20autoencoder/image-1.png)
    
    - ì¼ë°˜ì ì¸ autoencoderëŠ” ì…ë ¥ê³¼ ë³µì›ëœ ì¶œë ¥ì˜ ì°¨ì´ì¸ ì¬êµ¬ì„± ì˜¤ë¥˜ë§Œ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ. í•˜ì§€ë§Œ sparse autoencoderëŠ” ì†ì‹¤í•¨ìˆ˜ì— í¬ì†Œì„± íŒ¨ë„í‹°ë¥¼ ì¶”ê°€
        - í¬ì†Œì„± íŒ¨ë„í‹° : ì ì¬ ê³µê°„ ë‰´ëŸ°ë“¤ì˜ í‰ê·  í™œì„±í™” ì •ë„ê°€ ê°€ì¥ ë‚®ì€ ê°’ì— ê°€ê¹ë„ë¡ ìœ ë„. íŒ¨ë„í‹°ë¥¼ í†µí•´ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ì„ í¬ì°©í•˜ëŠ” ì†Œìˆ˜ì˜ ë‰´ëŸ°ì— ì˜ì¡´í•˜ë„ë¡ ê°•ì œ.
2. Denoising Autoencoder : ì…ë ¥ ë°ì´í„°ì— random noiseë‚˜ dropoutì„ ì¶”ê°€í•˜ëŠ” ê·œì œ ê¸°ë²• ì ìš©. ì…ë ¥ ë°ì´í„°ì— ì–´ë– í•œ ë…¸ì´ì¦ˆë¥¼ ë¶€ì—¬í•˜ë”ë¼ë„ manifold ìƒì—ì„œ ê°™ì€ ê³³ì— ìœ„ì¹˜í•´ì•¼ í•œë‹¤ëŠ” ê°€ì •. ì…ë ¥ë°ì´í„°ì— ì‘ì€ ë³€í™”ë¥¼ ì£¼ì–´ ì†ìƒëœ ë°ì´í„°ë¥¼ ë§Œë“¤ê³  ëª¨ë¸ì„ í†µí•´ ì†ìƒë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ ì¶œë ¥í•˜ëŠ” ë°©ë²•. ëœ ë¯¼ê°í•œ ê°•ê±´í•œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ  
    
    ![image.png](https://yunseo10987.github.io/assets/images/posts_img/2025-11-04%20autoencoder/image-2.png)
    
3. Contractive Autoencoder : ì‘ì€ ë³€í™”ì— ê°•ê±´í•œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©ì . DAEëŠ” ì…ë ¥ ë°ì´í„°ì˜ ì‘ì€ë³€í™”ì— ì €í•­í•˜ëŠ” ê²Œ ì¤‘ì ì´ë¼ë©´ CAEëŠ” ì¸ì½”ë”ê°€ ë””ì½”ë”ì—ì„œ ì¬êµ¬ì„±í•  ë•Œ ì¤‘ìš”í•˜ì§€ ì•Šì€ ì…ë ¥ì˜ ë³€í™”ë¥¼ ë¬´ì‹œí•˜ë„ë¡ í•˜ì—¬ íŠ¹ì§•ì„ ì¶”ì¶œí•  ë•Œ ì‘ì€ ë³€í™”ì— ëœ ë¯¼ê°í•˜ë„ë¡ ì¤‘ì   

### Application

- classification : autoencoderë¥¼ ì´ìš©í•´ í•™ìŠµì‹œí‚¨ Encoderë¥¼ Feature extractorë¡œ ì‚¬ìš©í•˜ì—¬ self-supervised learning í˜¹ì€ semi-supervised learningì— í™œìš©
- clustering : unlabeled data ê°„ì—ëŠ” ë§¤ë‹ˆí´ë“œê°€ ì¡´ì¬
- ì´ìƒì¹˜ íƒì§€ : ì •ìƒ ê´€ì¸¡ì¹˜ë§Œì„ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ì—¬ autoencoderê°€ ì´ìƒ ê´€ì¸¡ì¹˜ëŠ” ì˜ ë³µì›í•˜ì§€ ëª»í•  ê²ƒì´ë¼ëŠ” ê¸°ëŒ€ â†’ reconstruction lossê°€ í´ ê²ƒì´ë¼ ìƒê°
- ì°¨ì› ì¶•ì†Œ

```bash
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.utils.data import random_split
import matplotlib.pyplot as plt
import numpy as np

import os
os.environ["KMP_DUPLICATE_LIT_OK"]="TRUE"

# ì˜¤í† ì¸ì½”ë” ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±
class Autoencoder(nn.Module):
	def __init__(self):
		super(Autoencoder, self).__init__()
		
		#Encoder
		self.encoder = nn.Sequential(
			nn.Linear(28*28, 128),
			nn.ReLU(),
			nn.Linear(128, 64),
			nn.ReLU(),
			nn.Linear(64, 16),
			nn.ReLU(),
			nn.Linear(16, 2)	# ì ì¬ê³µê°„ latent space
		)
		
		#Decoder
		self.decoder = nn.Sequential(
			nn.Linear(2, 16),
			nn.ReLU(),
			nn.Linear(16, 64),
			nn.ReLU(),
			nn.Linear(64, 128),
			nn.ReLU(),
			nn.Linear(128, 28*28),
			nn.Sigmoid()
		)
	# ìˆœì „íŒŒ
	def forward(self, x):
		x_encoded = self.encoder(x) # x_encoded: ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ 784ì°¨ì›ì„ ì ì¬ ì°¨ì›ìœ¼ë¡œ ì¤„ì¸ ê°’
		x = self.decoder(x_encoded)
		return x, x_encoded

# =============================
# --- MNIST ë°ì´í„°ì…‹ í•™ìŠµ ì¤€ë¹„ ---
# =============================
# ë°ì´í„° ë³€í™˜
transform = transforms.Compose([
	transforms.ToTensor(),
])

# ì›ë³¸ í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ
full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)

# í•™ìŠµ ë°ì´í„°ì…‹ì„ train, validateë¡œ ë¶„ë¦¬ (8:2)
train_size = int(0.8 * len(full_train_dataset))
val_size = len(full_train_dataset) - train_size
train_subset, val_subset = random_split(full_train_dataset, [train_size, val_size])

# í•™ìŠµë°ì´í„°, ê²€ì¦ë°ì´í„° ë¡œë“œ 
train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=1000, shuffle=False)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=True)

# ============================
# --- AutoEncoder í•™ìŠµ ì§„í–‰ ---
# ============================
print(f"\n===== Training Autoencoder with 2D Latent Space =====")

# ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
model = Autoencoder()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
num_epochs = 60

train_losses = []
val_losses = []

# í•™ìŠµ ë£¨í”„
for epoch in range(num_epochs):
    model.train()
    running_train_loss = 0.0
    for data in train_loader:
        img, _ = data
        img = img.view(img.size(0), -1)
        output, _ = model(img)
        loss = criterion(output, img)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        running_train_loss += loss.item()
    
    epoch_train_loss = running_train_loss / len(train_loader)
    train_losses.append(epoch_train_loss)

    # --- ê²€ì¦ ë‹¨ê³„ ---
    model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for data in val_loader:
            img, _ = data
            img = img.view(img.size(0), -1)
            output, _ = model(img)
            loss = criterion(output, img)
            running_val_loss += loss.item()
    
    epoch_val_loss = running_val_loss / len(val_loader)
    val_losses.append(epoch_val_loss)

    # ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ê³¼ ê²€ì¦ ìˆ˜ì¹˜ ì¶œë ¥
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')

print("Training Complete")

# ì†ì‹¤ ì‹œê°í™”
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Training & Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# ==========================================================
# --- ëª¨ë¸ ë³µì›ë„ í‰ê°€ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì „ì²´ì— ëŒ€í•œ í‰ê·  ì†ì‹¤ ê³„ì‚°) ---
# ==========================================================
model.eval()
test_loss = 0.0
with torch.no_grad():
    for data in test_loader:
        imgs, _ = data
        imgs = imgs.view(imgs.size(0), -1)
        outputs, _ = model(imgs)
        loss = criterion(outputs, imgs)
        test_loss += loss.item()
test_loss /= len(test_loader)
print(f"\n--- Results for 2D Model ---")
print(f"ë³µì›ë„ (Average Test Loss): {test_loss:.4f}\n")

print(f"Visualizing reconstructed images for 2D model...")

# ================================
# --- ì‹¤ì œ ë°ì´í„°ì…‹ì„ ì´ìš©í•œ í…ŒìŠ¤íŠ¸ ---
# ================================
# í…ŒìŠ¤íŠ¸ ë°ì´í„°
dataiter = iter(test_loader)
images, _ = next(dataiter)
images_flattened = images.view(images.size(0), -1)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¨ AutoEncoderë¥¼ í†µí•´ í…ŒìŠ¤íŠ¸
output, _ = model(images_flattened)
output = output.view(output.size(0), 1, 28, 28).detach()

# ê²°ê³¼ ì‹œê°í™”
fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))
fig.suptitle(f'Reconstruction Results (2D Latent Space)')
# ì›ë³¸ ì´ë¯¸ì§€
for i in range(10):
    img = images[i].squeeze()
    axes[0,i].imshow(img.numpy(), cmap='gray')
    axes[0,i].get_xaxis().set_visible(False)
    axes[0,i].get_yaxis().set_visible(False)
    if i == 0: axes[0,i].set_title('Originals', loc='left')

# ë³µì›ëœ ì´ë¯¸ì§€
for i in range(10):
    img = output[i].squeeze()
    axes[1,i].imshow(img.numpy(), cmap='gray')
    axes[1,i].get_xaxis().set_visible(False)
    axes[1,i].get_yaxis().set_visible(False)
    if i == 0: axes[1,i].set_title('Reconstructed', loc='left')
plt.show()

# ============================
# --- latent vactors ì‹œê°í™” ---
# ============================
model.eval()	# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ë³€ê²½

latent_vectors = []
labels = []

with torch.no_grad():
	for data in test_loader:
		imgs, lbls = data
		imgs = imgs.view(imgs.size(0), -1)
		_, latents = model(imgs)
		latent_vectors.append(latents)
		labels.append(lbls)
		
latent_vectors = torch.cat(latent_vectors, dim=0)
labels = torch.cat(labels, dim=0)

# 2D ì‹œê°í™”
plt.figure(figsize=(12, 10))
# ê° ìˆ«ì ë ˆì´ë¸”(0-9)ì— ëŒ€í•´ ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ ì‚°ì ë„ ê·¸ë¦¬ê¸°
for i in range(10):
	indices = labels == i
	plt.scatter(latent_vectors[indices, 0], latent_vectors[indices, 1], label=str(i), alpha=0.7, s=15)

plt.xlabel('Latent X')
plt.ylabel('Latent Y')
plt.title('2D Latent Space Visualization')
plt.legend()
plt.grid(True)
plt.show()

# =======================================
# --- 3ì°¨ì› ì´ìƒì˜ latent vector í™•ì¸ ì‹œ ---
# =======================================
"""
from sklearn.manifold import TSNE

# t-SNE ì ìš©
tsne = TSNE(n_components=2, perplexity=30, max_iter=300)
tsne_result = tsne.fit_transform(latent_vectors)

# ì‹œê°í™”
plt.figure(figsize=(20, 8))

# t-SNE ê²°ê³¼ í”Œë¡¯
for i in range(10):
    indices = labels == i
    plt.scatter(tsne_result[indices, 0], tsne_result[indices, 1], label=str(i), alpha=0.7, s=15)
plt.title('t-SNE Visualization of Latent Space')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.legend()
plt.grid(True)

plt.show()
"""
```

- `def __init__(self):` ìƒì„±ì
    - `super(Autoencoder, self).__init__()` : ë¶€ëª¨ ìƒì„±ì ìƒì†, ëª…ì‹œì ìœ¼ë¡œ í•´ë‹¹ ê°ì²´ì™€ í´ë˜ìŠ¤ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë„£ìŒ
    - `self.encoder, self.decoder`: ì¸ì½”ë” ë° ë””ì½”ë”
        - `nn.Sequential()` : ì—¬ëŸ¬ ê°œì˜ ì‹ ê²½ë§ ë ˆì´ì–´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¬¶ì–´ í•˜ë‚˜ì˜ ëª¨ë“ˆë¡œ ë§Œë“œëŠ” ë° ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤ì´ì ë©”ì†Œë“œ
        - `nn.Linear()` : ì…ë ¥ ë°ì´í„°ë¥¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ê³¼ ê³±í•˜ê³  í¸í–¥ì„ ë”í•˜ëŠ” ì„ í˜• ë³€í™˜ì„ ìˆ˜í–‰. í•´ë‹¹ íŒŒë¼ë¯¸í„°ë§Œí¼ ì••ì¶• 784 â†’ 128 â†’ 64 â†’ 16 â†’ 2
        - `nn.ReLU()` : ì…ë ¥ì´ 0ë³´ë‹¤ í¬ë©´ ê·¸ëŒ€ë¡œ ì¶œë ¥, 0ë³´ë‹¤ ì‘ìœ¼ë©´ 0 ì¶œë ¥
        - `nn.Sigmoid()` : ì…ë ¥ì„ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì••ì¶•
- `def forward(self, x)` : ì¸ì½”ë”© - ë””ì½”ë”© ê³¼ì •ì„ ìˆ˜í–‰
    - `x` : í…ì„œê°’
    - `x_encoded` : ì ì¬ ì°¨ì›ìœ¼ë¡œ ì••ì¶•í•œ ê°’
- `transform` : í•¨ìˆ˜ì²˜ëŸ¼ ì‘ë™í•˜ëŠ” ê°ì²´. callable object
    - `transforms.Compose()` : ì—¬ëŸ¬ ê°œì˜ ë³€í™˜ ì‘ì—…ì„ í•˜ë‚˜ì˜ ìˆœì°¨ì ì¸ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¬¶ì–´ì¤Œ
    - `transforms.ToTensor()` : ì´ë¯¸ì§€ë¥¼ í…ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- `full_train_dataset` : í˜¸ì¶œë˜ì–´ ë§Œë“¤ì–´ì§„ ê°ì²´ëŠ” ë°ì´í„°ì…‹ì˜ ì „ì²´ í•™ìŠµ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë°”ë¡œ ì˜¬ë¦¬ëŠ” ëŒ€ì‹  ì¸í„°í˜ì´ìŠ¤ë¥¼ í• ë‹¹. íŒŒë¼ë¯¸í„°ë¡œ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë¶ˆëŸ¬ì˜¬ì§€ ê²°ì •
    - `transform` ê°ì²´ë¥¼ í†µí•´ ì „ì²˜ë¦¬
- `train_loader, val_loader` : í•™ìŠµ ë°ì´í„°, ê²€ì¦ ë°ì´í„° ë¡œë“œ
    - `DataLoader()` : ë°ì´í„° ë¡œë” í´ë˜ìŠ¤, ë°ì´í„° ê°ì²´ë¥¼ ë°›ì•„ ëª¨ë¸ì´ ì²˜ë¦¬í•˜ê¸° ì‰¬ìš´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì œê³µí•´ì£¼ëŠ” ë°˜ë³µìë¥¼ ìƒì„±
- `Optimizer` : ëª¨ë¸ì´ ê³„ì‚°í•œ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´, ê° ê°€ì¤‘ì¹˜ë¥¼ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ í¬ê²Œ ì¡°ì •í•´ì•¼í•˜ëŠ” ì§€ ê²°ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
- í•™ìŠµ ë£¨í”„
    - `epoch` : ì „ì²´ ë°ì´í„° ì…‹ì„ í•™ìŠµí•˜ëŠ” íšŸìˆ˜
    - `model.train()` : í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •. í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •í•´ì•¼ ê³„ì‚° ë° ì—…ë°ì´íŠ¸ë¥¼ í™œì„±í™”
    - `for data in train_loader` : DataLoader()ë¥¼ í†µí•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê²Œ ë¨
        - `img, _ = data` : `data`ëŠ” 32ê°œì˜ ì´ë¯¸ì§€ì™€ 32ê°œì˜ ë ˆì´ë¸”ì„ ë‹´ê³  ìˆëŠ” íŠœí”Œ í˜•íƒœ. _ë¥¼ í†µí•´ ì´ë¯¸ì§€ë§Œ imgì— í• ë‹¹ë˜ê³  ë ˆì´ë¸”ì€ ë¬´ì‹œë¨.
        - `img = img.view(img.size(0), -1)` : nn.Linear()ì— ì…ë ¥ë˜ê¸° ìœ„í•´ í‰íƒ„í™”
        - outputê³¼ imgë¥¼ ì†ì‹¤ í•¨ìˆ˜ì— ë„£ì–´ì„œ ì¬êµ¬ì„± ì˜¤ë¥˜ë¥¼ ê³„ì‚°.
        - `optimizer.zero_grad()` : ì´ì „ ì´í„°ë ˆì´ì…˜ì—ì„œ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ì˜ ê¸°ìš¸ê¸°ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        - `loss.backward()` : ì—­ì „íŒŒë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ê°€ ì˜¤ë¥˜ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ ê¸°ìš¸ê¸° ê³„ì‚°
        - `optimizer.step()` : ê³„ì‚°ëœ ê¸°ìš¸ê¸°ì™€ í•™ìŠµë¥ ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ ê³„ì‚°
        - `running_train_loss += loss.item()` : í•´ë‹¹ ì—í¬í¬ì˜ ì´ ì†ì‹¤ë¥ ì„ êµ¬í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ì—ì„œ ì†ì‹¤ë¥  í•©í•¨
    - `epoch_train_loss` : í•´ë‹¹ ì—í¬í¬ì˜ í‰ê·  ì†ì‹¤ì„ ê³„ì‚°í•˜ê³  ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ìœ„í•´ í‰ê·  ì†ì‹¤ê°’ì„ ëª©ë¡ì— ì¶”ê°€
    - ê²€ì¦ ë‹¨ê³„ë¥¼ í†µí•´ ì—í¬í¬ë§ˆë‹¤ ê²€ì¦ì„ ì§„í–‰

## ExplainableBug

- ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ì…‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.manifold import TSNE
import argparse
import os
from sklearn.ensemble import IsolationForest

# KMP_DUPLICATE_LIT_OK ì„¤ì • (matplotlib/intel ì¶©ëŒ ë°©ì§€)
os.environ["KMP_DUPLICATE_LIT_OK"]="TRUE"

class NetworkTrafficDataset(Dataset):
    """CSV ë°ì´í„°ë¥¼ ìœ„í•œ PyTorch ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹"""
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.int64)
    def __len__(self):
        return len(self.y)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx] 
```

`__init__` : ë°ì´í„° xì™€ ë ˆì´ë¸” yë¥¼ ì¸ìˆ˜ë¡œ ë°›ì•„ Tensorë¡œ ë³€í™˜

`__len__` : ë°ì´í„°ì…‹ ì „ì²´ í¬ê¸°. Xì™€ yì˜ ê°œìˆ˜ê°€ ê°™ìœ¼ë¯€ë¡œ self.Xë¡œ í•´ë„ ìƒê´€ ì—†ìŒ.

`__getitem__` : íŠ¹ì • idxì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œì„ í•˜ë‚˜ í˜¸ì¶œ.

- AutoEncoder ëª¨ë¸

```python
class Autoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),      # ğŸ”¹ ì¶”ê°€
            nn.Linear(64, 16),
            nn.ReLU(),
            nn.Linear(16, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 64),
            nn.ReLU(),
            nn.Dropout(0.2),      # ğŸ”¹ ì¶”ê°€
            nn.Linear(64, input_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z
```

`input_dim` : ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì„ 64ì°¨ì›ìœ¼ë¡œ ì„ í˜• ë³€í™˜

`Dropout(0.2)`  : í›ˆë ¨ ì¤‘ 20%ì˜ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ë¹„í™œì„±í™”í•˜ì—¬ ê³¼ì í•© ë°©ì§€

- ì „ì²˜ë¦¬ í•¨ìˆ˜

```python
def preprocess_csv(file_path):
    """CSV íŒŒì¼ì„ ì½ê³  NaN/Infë¥¼ ì²˜ë¦¬í•œ ë’¤ íŠ¹ì§•(X)ì„ ë°˜í™˜"""
    df = pd.read_csv(file_path)
    
    identifier_cols = ['filename', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol']
    
    cols_to_drop = [col for col in identifier_cols if col in df.columns]
    
    # ì‹ë³„ì ì—´ì„ ì œì™¸í•œ ìˆœìˆ˜ íŠ¹ì§•(feature)ë§Œ ë‚¨ê¹€
    df_features = df.drop(columns=cols_to_drop)
    
    # 'filename' ì—´ì´ ìˆë‹¤ë©´ ì œê±° (íŠ¹ì§•ì´ ì•„ë‹˜)
    if 'filename' in df_features.columns:
        df_features = df_features.drop(columns=['filename'])
        
    # ë¬´í•œëŒ€(Inf) ê°’ ë° NaN ì²˜ë¦¬
    df_features.replace([np.inf, -np.inf], np.nan, inplace=True)
    
    # NaN ê°’ì„ í•´ë‹¹ ì—´ì˜ í‰ê· (mean)ìœ¼ë¡œ ëŒ€ì²´
    for col in df_features.columns:
        if df_features[col].isna().any():
            mean_val = df_features[col].mean()
            df_features[col].fillna(mean_val, inplace=True)
            
    return df_features, df
```

`df = pd.read_csv()` : csv íŒŒì¼ ê²½ë¡œë¥¼ ì½ì–´ì„œ DataFrameìœ¼ë¡œ ë©”ëª¨ë¦¬ì— ë¡œë“œ

`identifier_cols = []` : ì‹ë³„ì íŠ¹ì§• 

`cols_to_drop` : DataFrameì˜ featureë“¤ ì¤‘ ì‹ë³„ì feature í•„í„°ë§

`df_features` : ì‹ë³„ì featureë¥¼ ì œê±°í•œ DataFrame 

`if` : ì œê±° ì•ˆë˜ë©´ ë‹¤ì‹œ ì œê±° 

`df_features.replace` : ì–‘/ìŒì˜ ë¬´í•œëŒ€ë¥¼ ê²°ì¸¡ê°’ nanìœ¼ë¡œ ì²˜ë¦¬

`for()` : ì—´ì„ ëŒë©´ì„œ ë§Œì•½ ì—´ ì¤‘ì— nan ê°’ì´ ìˆë‹¤ë©´ í•´ë‹¹ ì—´ì˜ í‰ê·  ê°’ìœ¼ë¡œ ëŒ€ì²´

- ë©”ì¸ í•¨ìˆ˜

```python
def main():
    parser = argparse.ArgumentParser(description="Train Autoencoder (PyTorch) for Anomaly Detection.")
    parser.add_argument('-n', '--normal_csv', required=True, 
                        help="Path to the NORMAL traffic CSV file (e.g., all_features.csv).")
    parser.add_argument('-m', '--malicious_csv', required=True, 
                        help="Path to the MALICIOUS traffic CSV file (e.g., malware-traffic-features.csv).")
    args = parser.parse_args()

    # --- 1. ë°ì´í„° ë¡œë“œ ---
    print(f"Loading normal data from: {args.normal_csv}")
    X_normal_df, _ = preprocess_csv(args.normal_csv) # ì •ìƒ ë°ì´í„°ëŠ” ì›ë³¸ ì‹ë³„ì ë¶ˆí•„ìš”
    
    print(f"Loading malicious data from: {args.malicious_csv}")
    X_malicious_df, X_malicious_df_orig = preprocess_csv(args.malicious_csv)

    # --- 2. íŠ¹ì§•(ì»¬ëŸ¼) ì •ë ¬ (ì¤‘ìš”) ---
    # ë‘ CSVì˜ ì»¬ëŸ¼ ìˆœì„œì™€ ê°œìˆ˜ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶¤
    common_cols = list(set(X_normal_df.columns) & set(X_malicious_df.columns))
    print(f"Found {len(common_cols)} common features.")
    
    X_normal_df = X_normal_df[common_cols]
    X_malicious_df = X_malicious_df[common_cols]
    
    input_dim = len(common_cols) # Autoencoder ì…ë ¥ ì°¨ì›

    # Pandas DataFrameì„ Numpy ë°°ì—´ë¡œ ë³€í™˜
    X_normal_raw = X_normal_df.values
    X_malicious_raw = X_malicious_df.values
    
    print(f"Normal samples: {len(X_normal_raw)}")
    print(f"Malicious samples: {len(X_malicious_raw)}")
```

`parser ~ arg` : ì‹¤í–‰ì‹œí‚¬ ë•Œ ë§Œë“œëŠ” ì¸ì ë°›ê³  ë°°ì—´í™”

`common_cols` : ì •ìƒ ë°ì´í„°ì™€ ì•…ì„± ë°ì´í„°ì˜ ê³µí†µ ì—´ ì¶”ì¶œ

`X_normal_df = X_normal_df[common_cols]` : DataFrame ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë¬¸ë²•ì— ë”°ë¼ [] ì•ˆì— ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ê°€ë©´ í•´ë‹¹ ì—´ë¡œë§Œ í•„í„°ë§

`X_normal_raw = X_normal_df.values` : value ë“¤ì„ 2ì°¨ì› ë°°ì—´ë¡œ ë³€ê²½

```python
 # --- 3. ë°ì´í„°ì…‹ ë¶„ë¦¬ (í•µì‹¬ ë¡œì§) ---
    
    # 3.1. ì •ìƒ ë°ì´í„°ë¥¼ í›ˆë ¨(90%) / í…ŒìŠ¤íŠ¸(10%)ìš©ìœ¼ë¡œ ë¶„ë¦¬
    X_train_val_normal_raw, X_test_normal_raw = train_test_split(X_normal_raw, test_size=0.1, random_state=42)
    
    # 3.2. í›ˆë ¨ìš©(90%) ë°ì´í„°ë¥¼ ë‹¤ì‹œ í›ˆë ¨(90%) / ê²€ì¦(10%)ìš©ìœ¼ë¡œ ë¶„ë¦¬
    # (164k * 0.9) * 0.9 = ~133k (Train)
    # (164k * 0.9) * 0.1 = ~15k (Validation)
    X_train_normal_raw, X_val_normal_raw = train_test_split(X_train_val_normal_raw, test_size=0.1, random_state=42)

    # --- 4. ìŠ¤ì¼€ì¼ë§ (ì¤‘ìš”) ---
    # Scalerë¥¼ ì˜¤ì§ 'ì •ìƒ í›ˆë ¨ ë°ì´í„°'ë¡œë§Œ fit
    scaler = StandardScaler()
    scaler.fit(X_train_normal_raw)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©
    X_train_scaled = scaler.transform(X_train_normal_raw)
    X_val_scaled = scaler.transform(X_val_normal_raw)
    X_test_normal_scaled = scaler.transform(X_test_normal_raw)
    X_malicious_scaled = scaler.transform(X_malicious_raw)
    
    print(f"Training samples (Normal): {len(X_train_scaled)}")
    print(f"Validation samples (Normal): {len(X_val_scaled)}")

    # --- 5. PyTorch ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„± ---
    
    # 5.1. í•™ìŠµìš© (Train/Validation) ë¡œë” (ì˜¤ì§ ì •ìƒ ë°ì´í„°)
    train_dataset = NetworkTrafficDataset(X_train_scaled, np.zeros(len(X_train_scaled)))
    val_dataset = NetworkTrafficDataset(X_val_scaled, np.zeros(len(X_val_scaled)))
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)

    # 5.2. ìµœì¢… í‰ê°€ìš© (Test) ë¡œë” (ì •ìƒ 10% + ì•…ì„± 100%)
    # ë¶„ë¦¬í•´ë‘” ì •ìƒ í…ŒìŠ¤íŠ¸ì…‹ê³¼ ì•…ì„± í…ŒìŠ¤íŠ¸ì…‹ì„ í•©ì¹¨
    X_test_combined = np.concatenate((X_test_normal_scaled, X_malicious_scaled), axis=0)
    
    # ë ˆì´ë¸” ìƒì„± (ì •ìƒ: 0, ì•…ì„±: 1)
    y_test_normal = np.zeros(len(X_test_normal_scaled))
    y_malicious = np.ones(len(X_malicious_scaled))
    y_test_combined = np.concatenate((y_test_normal, y_malicious), axis=0)
    
    test_dataset = NetworkTrafficDataset(X_test_combined, y_test_combined)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
    
    print(f"Test samples: {len(X_test_combined)} (Normal: {len(y_test_normal)}, Malicious: {len(y_malicious)})")

    # ===============================
    # --- 6. AutoEncoder í•™ìŠµ ì§„í–‰ ---
    # ===============================

    output_dim = 4
    print(f"\n===== Training Autoencoder with {input_dim}D Input -> {output_dim}D Latent Space =====")    

    model = Autoencoder(input_dim=input_dim, latent_dim=output_dim)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5) 
    num_epochs = 45 

    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        model.train()
        running_train_loss = 0.0
        for features, _ in train_loader:
            output, _ = model(features)
            loss = criterion(output, features)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            running_train_loss += loss.item()
        
        epoch_train_loss = running_train_loss / len(train_loader)
        train_losses.append(epoch_train_loss)

        # --- ê²€ì¦ ë‹¨ê³„ ---
        model.eval()
        running_val_loss = 0.0
        with torch.no_grad():
            for features, _ in val_loader:
                output, _ = model(features)
                loss = criterion(output, features)
                running_val_loss += loss.item()
        
        epoch_val_loss = running_val_loss / len(val_loader)
        val_losses.append(epoch_val_loss)

        if (epoch+1) % 5 == 0: # 5 ì—í¬í¬ë§ˆë‹¤ ì¶œë ¥
            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.6f}, Val Loss: {epoch_val_loss:.6f}')

    print("Training Complete")

    # í•™ìŠµ ì™„ë£Œ í›„
    print("Training Complete... Saving model.")
    torch.save(model.state_dict(), "autoencoder_model.pth")

if __name__ == "__main__":
    main()
```