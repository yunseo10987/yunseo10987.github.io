---
title: "[2025-11-26] AutoEncoder ì½”ë“œ ë¶„ì„"
excerpt: "ì„¤ëª…ì¶© í”„ë¡œì íŠ¸"

categories:
  - Project
tags:
  - [NetWork]

permalink: /Project/[2025-11-26] AutoEncoder ì½”ë“œ ë¶„ì„/

toc: true
toc_sticky: true

date: 2025-11-26
last_modified_at: 2025-11-26
---

## ExplainableBug

- ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ì…‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.manifold import TSNE
import argparse
import os
from sklearn.ensemble import IsolationForest

# KMP_DUPLICATE_LIT_OK ì„¤ì • (matplotlib/intel ì¶©ëŒ ë°©ì§€)
os.environ["KMP_DUPLICATE_LIT_OK"]="TRUE"

class NetworkTrafficDataset(Dataset):
    """CSV ë°ì´í„°ë¥¼ ìœ„í•œ PyTorch ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹"""
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.int64)
    def __len__(self):
        return len(self.y)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx] 
```

`__init__` : ë°ì´í„° xì™€ ë ˆì´ë¸” yë¥¼ ì¸ìˆ˜ë¡œ ë°›ì•„ Tensorë¡œ ë³€í™˜

- ë°ì´í„°
    
    
    | **ìƒ˜í”Œ ID** | **Feature 1: Duration (sec)** | **Feature 2: Byte_Count** | **Feature 3: Packets** |
    | --- | --- | --- | --- |
    | **0** | 0.5 | 1200 | 10 |
    | **1** | 1.2 | 980 | 8 |
    | **2** | 20.1 | 5500 | 50 |
    
    ```python
    X_numpy = [
        [0.5, 1200, 10],   # ìƒ˜í”Œ 0
        [1.2, 980, 8],     # ìƒ˜í”Œ 1
        [20.1, 5500, 50]   # ìƒ˜í”Œ 2
    ]
    ```
    
- ë ˆì´ë¸”
    
    
    | **ìƒ˜í”Œ ID** | **ë ˆì´ë¸” (Y)** | **ì˜ë¯¸** |
    | --- | --- | --- |
    | **0** | 0 | ì •ìƒ íŠ¸ë˜í”½ |
    | **1** | 0 | ì •ìƒ íŠ¸ë˜í”½ |
    | **2** | 0 | ì •ìƒ íŠ¸ë˜í”½ |
    
    ```python
    y_numpy = [0, 0, 0]
    ```
    

`__len__` : ë°ì´í„°ì…‹ ì „ì²´ í¬ê¸°. Xì™€ yì˜ ê°œìˆ˜ê°€ ê°™ìœ¼ë¯€ë¡œ self.Xë¡œ í•´ë„ ìƒê´€ ì—†ìŒ.

`__getitem__` : íŠ¹ì • idxì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œì„ í•˜ë‚˜ í˜¸ì¶œ.

- AutoEncoder ëª¨ë¸

```python
class Autoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),      # ğŸ”¹ ì¶”ê°€
            nn.Linear(64, 16),
            nn.ReLU(),
            nn.Linear(16, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 64),
            nn.ReLU(),
            nn.Dropout(0.2),      # ğŸ”¹ ì¶”ê°€
            nn.Linear(64, input_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat, z
```

`input_dim` : ì…ë ¥ ë°ì´í„°ì˜ ì°¨ì›ì„ 64ì°¨ì›ìœ¼ë¡œ ì„ í˜• ë³€í™˜

`Dropout(0.2)`  : í›ˆë ¨ ì¤‘ 20%ì˜ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ë¹„í™œì„±í™”í•˜ì—¬ ê³¼ì í•© ë°©ì§€

- ì „ì²˜ë¦¬ í•¨ìˆ˜

```python
def preprocess_csv(file_path):
    """CSV íŒŒì¼ì„ ì½ê³  NaN/Infë¥¼ ì²˜ë¦¬í•œ ë’¤ íŠ¹ì§•(X)ì„ ë°˜í™˜"""
    df = pd.read_csv(file_path)
    
    identifier_cols = ['filename', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol']
    
    cols_to_drop = [col for col in identifier_cols if col in df.columns]
    
    # ì‹ë³„ì ì—´ì„ ì œì™¸í•œ ìˆœìˆ˜ íŠ¹ì§•(feature)ë§Œ ë‚¨ê¹€
    df_features = df.drop(columns=cols_to_drop)
    
    # 'filename' ì—´ì´ ìˆë‹¤ë©´ ì œê±° (íŠ¹ì§•ì´ ì•„ë‹˜)
    if 'filename' in df_features.columns:
        df_features = df_features.drop(columns=['filename'])
        
    # ë¬´í•œëŒ€(Inf) ê°’ ë° NaN ì²˜ë¦¬
    df_features.replace([np.inf, -np.inf], np.nan, inplace=True)
    
    # NaN ê°’ì„ í•´ë‹¹ ì—´ì˜ í‰ê· (mean)ìœ¼ë¡œ ëŒ€ì²´
    for col in df_features.columns:
        if df_features[col].isna().any():
            mean_val = df_features[col].mean()
            df_features[col].fillna(mean_val, inplace=True)
            
    return df_features, df
```

`df = pd.read_csv()` : csv íŒŒì¼ ê²½ë¡œë¥¼ ì½ì–´ì„œ DataFrameìœ¼ë¡œ ë©”ëª¨ë¦¬ì— ë¡œë“œ

`identifier_cols = []` : ì‹ë³„ì íŠ¹ì§• 

`cols_to_drop` : DataFrameì˜ featureë“¤ ì¤‘ ì‹ë³„ì feature í•„í„°ë§

`df_features` : ì‹ë³„ì featureë¥¼ ì œê±°í•œ DataFrame 

`if` : ì œê±° ì•ˆë˜ë©´ ë‹¤ì‹œ ì œê±° 

`df_features.replace` : ì–‘/ìŒì˜ ë¬´í•œëŒ€ë¥¼ ê²°ì¸¡ê°’ nanìœ¼ë¡œ ì²˜ë¦¬

`for()` : ì—´ì„ ëŒë©´ì„œ ë§Œì•½ ì—´ ì¤‘ì— nan ê°’ì´ ìˆë‹¤ë©´ í•´ë‹¹ ì—´ì˜ í‰ê·  ê°’ìœ¼ë¡œ ëŒ€ì²´

- ë©”ì¸ í•¨ìˆ˜

```python
def main():
    parser = argparse.ArgumentParser(description="Train Autoencoder (PyTorch) for Anomaly Detection.")
    parser.add_argument('-n', '--normal_csv', required=True, 
                        help="Path to the NORMAL traffic CSV file (e.g., all_features.csv).")
    parser.add_argument('-m', '--malicious_csv', required=True, 
                        help="Path to the MALICIOUS traffic CSV file (e.g., malware-traffic-features.csv).")
    args = parser.parse_args()

    # --- 1. ë°ì´í„° ë¡œë“œ ---
    print(f"Loading normal data from: {args.normal_csv}")
    X_normal_df, _ = preprocess_csv(args.normal_csv) # ì •ìƒ ë°ì´í„°ëŠ” ì›ë³¸ ì‹ë³„ì ë¶ˆí•„ìš”
    
    print(f"Loading malicious data from: {args.malicious_csv}")
    X_malicious_df, X_malicious_df_orig = preprocess_csv(args.malicious_csv)

    # --- 2. íŠ¹ì§•(ì»¬ëŸ¼) ì •ë ¬ (ì¤‘ìš”) ---
    # ë‘ CSVì˜ ì»¬ëŸ¼ ìˆœì„œì™€ ê°œìˆ˜ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶¤
    common_cols = list(set(X_normal_df.columns) & set(X_malicious_df.columns))
    print(f"Found {len(common_cols)} common features.")
    
    X_normal_df = X_normal_df[common_cols]
    X_malicious_df = X_malicious_df[common_cols]
    
    input_dim = len(common_cols) # Autoencoder ì…ë ¥ ì°¨ì›

    # Pandas DataFrameì„ Numpy ë°°ì—´ë¡œ ë³€í™˜
    X_normal_raw = X_normal_df.values
    X_malicious_raw = X_malicious_df.values
    
    print(f"Normal samples: {len(X_normal_raw)}")
    print(f"Malicious samples: {len(X_malicious_raw)}")
```

`parser ~ arg` : ì‹¤í–‰ì‹œí‚¬ ë•Œ ë§Œë“œëŠ” ì¸ì ë°›ê³  ë°°ì—´í™”

`common_cols` : ì •ìƒ ë°ì´í„°ì™€ ì•…ì„± ë°ì´í„°ì˜ ê³µí†µ ì—´ ì¶”ì¶œ

`X_normal_df = X_normal_df[common_cols]` : DataFrame ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë¬¸ë²•ì— ë”°ë¼ [] ì•ˆì— ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ê°€ë©´ í•´ë‹¹ ì—´ë¡œë§Œ í•„í„°ë§

`X_normal_raw = X_normal_df.values` : value ë“¤ì„ 2ì°¨ì› ë°°ì—´ë¡œ ë³€ê²½

```python
 # --- 3. ë°ì´í„°ì…‹ ë¶„ë¦¬ (í•µì‹¬ ë¡œì§) ---
    
    # 3.1. ì •ìƒ ë°ì´í„°ë¥¼ í›ˆë ¨(90%) / í…ŒìŠ¤íŠ¸(10%)ìš©ìœ¼ë¡œ ë¶„ë¦¬
    X_train_val_normal_raw, X_test_normal_raw = train_test_split(X_normal_raw, test_size=0.1, random_state=42)
    
    # 3.2. í›ˆë ¨ìš©(90%) ë°ì´í„°ë¥¼ ë‹¤ì‹œ í›ˆë ¨(90%) / ê²€ì¦(10%)ìš©ìœ¼ë¡œ ë¶„ë¦¬
    # (164k * 0.9) * 0.9 = ~133k (Train)
    # (164k * 0.9) * 0.1 = ~15k (Validation)
    X_train_normal_raw, X_val_normal_raw = train_test_split(X_train_val_normal_raw, test_size=0.1, random_state=42)

```

`train_test_split()` : ë°ì´í„°ì…‹ì„ í›ˆë ¨ìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜. 

- `random_state`ë¥¼ ê³ ì •í•˜ë©´ ì–¸ì œ ì‹¤í–‰í•´ë„ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ìŒ.
- `shuffle` : ê¸°ë³¸ê°’ì€ Trueë¡œ, ë°ì´í„° ë¶„í• í•˜ê¸° ì „ ë¬´ì‘ìœ„ë¡œ ì„ì„ ì§€ë¥¼ ì§€ì •
- arrayë¥¼ ë‘ ê°œ ë„£ì–´ì„œ ë™ì‹œì— ë¶„í•  í•  ìˆ˜ë„ ìˆìŒ

```python
# --- 4. ìŠ¤ì¼€ì¼ë§ (ì¤‘ìš”) ---
    # Scalerë¥¼ ì˜¤ì§ 'ì •ìƒ í›ˆë ¨ ë°ì´í„°'ë¡œë§Œ fit
    scaler = StandardScaler()
    scaler.fit(X_train_normal_raw)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©
    X_train_scaled = scaler.transform(X_train_normal_raw)
    X_val_scaled = scaler.transform(X_val_normal_raw)
    X_test_normal_scaled = scaler.transform(X_test_normal_raw)
    X_malicious_scaled = scaler.transform(X_malicious_raw)
    
    print(f"Training samples (Normal): {len(X_train_scaled)}")
    print(f"Validation samples (Normal): {len(X_val_scaled)}")
```

`StandardScaler()` : í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ì¡°ì •í•˜ëŠ” ìŠ¤ì¼€ì¼ëŸ¬ 

`scaler.fit()` : ì •ìƒ í›ˆë ¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ scalerë¥¼ í•™ìŠµ. ìœ„ì˜ ì½”ë“œì—ì„œëŠ” ì •ìƒ í›ˆë ¨ ë°ì´í„°ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•œ í›„ `transform()` í•  ë•Œ, í•´ë‹¹ ê°’ë“¤ì„ ì‚¬ìš©í•˜ì—¬ í‘œì¤€í™”  

`scaler.transform()` : ê° ë°ì´í„°ë“¤ì„ í‘œì¤€í™”í•¨.

```python
# --- 5. PyTorch ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„± ---
    
    # 5.1. í•™ìŠµìš© (Train/Validation) ë¡œë” (ì˜¤ì§ ì •ìƒ ë°ì´í„°)
    train_dataset = NetworkTrafficDataset(X_train_scaled, np.zeros(len(X_train_scaled)))
    val_dataset = NetworkTrafficDataset(X_val_scaled, np.zeros(len(X_val_scaled)))
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)

    # 5.2. ìµœì¢… í‰ê°€ìš© (Test) ë¡œë” (ì •ìƒ 10% + ì•…ì„± 100%)
    # ë¶„ë¦¬í•´ë‘” ì •ìƒ í…ŒìŠ¤íŠ¸ì…‹ê³¼ ì•…ì„± í…ŒìŠ¤íŠ¸ì…‹ì„ í•©ì¹¨
    X_test_combined = np.concatenate((X_test_normal_scaled, X_malicious_scaled), axis=0)
    
    # ë ˆì´ë¸” ìƒì„± (ì •ìƒ: 0, ì•…ì„±: 1)
    y_test_normal = np.zeros(len(X_test_normal_scaled))
    y_malicious = np.ones(len(X_malicious_scaled))
    y_test_combined = np.concatenate((y_test_normal, y_malicious), axis=0)
    
    test_dataset = NetworkTrafficDataset(X_test_combined, y_test_combined)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
    
    print(f"Test samples: {len(X_test_combined)} (Normal: {len(y_test_normal)}, Malicious: {len(y_malicious)})")
```

`train_dataset` : `NetworkTrafficDataset` ì„ í†µí•´ Tensorë¡œ ë³€í™˜. ë ˆì´ë¸”ì€ í•´ë‹¹ ë°ì´í„°ì…‹ ê¸¸ì´ ë§Œí¼ ëª¨ë“  ìƒ˜í”Œì— 0ì„ ë¶€ì—¬

- í•™ìŠµ ì‹œì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ????

`train_loader` : ë°ì´í„°ë¥¼ ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ ë°˜ë³µí•˜ì—¬ ì œê³µ ë°›ìŒ. shuffleì„ í†µí•´ ë°ì´í„°ì˜ ìˆœì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ì–´ì„œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì„ 

```python

    # ===============================
    # --- 6. AutoEncoder í•™ìŠµ ì§„í–‰ ---
    # ===============================

    output_dim = 4
    print(f"\n===== Training Autoencoder with {input_dim}D Input -> {output_dim}D Latent Space =====")    

    model = Autoencoder(input_dim=input_dim, latent_dim=output_dim)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5) 
    num_epochs = 45 

    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        model.train()
        running_train_loss = 0.0
        for features, _ in train_loader:
            output, _ = model(features)
            loss = criterion(output, features)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            running_train_loss += loss.item()
        
        epoch_train_loss = running_train_loss / len(train_loader)
        train_losses.append(epoch_train_loss)

        # --- ê²€ì¦ ë‹¨ê³„ ---
        model.eval()
        running_val_loss = 0.0
        with torch.no_grad():
            for features, _ in val_loader:
                output, _ = model(features)
                loss = criterion(output, features)
                running_val_loss += loss.item()
        
        epoch_val_loss = running_val_loss / len(val_loader)
        val_losses.append(epoch_val_loss)

        if (epoch+1) % 5 == 0: # 5 ì—í¬í¬ë§ˆë‹¤ ì¶œë ¥
            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.6f}, Val Loss: {epoch_val_loss:.6f}')

    print("Training Complete")

    # í•™ìŠµ ì™„ë£Œ í›„
    print("Training Complete... Saving model.")
    torch.save(model.state_dict(), "autoencoder_model.pth")

if __name__ == "__main__":
    main()
```