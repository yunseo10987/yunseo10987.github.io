---
title: "[2025-12-22] 프로젝트 보고서"
excerpt: "설명충 프로젝트"

categories:
  - Project
tags:
  - [NetWork]

permalink: /Project/[2025-12-22] 프로젝트 보고서/

toc: true
toc_sticky: true

date: 2025-12-22
last_modified_at: 2025-12-22
---

프로젝트 보고서
1. 프로젝트 개요

항목
내용
프로젝트명
Explain Wall
보고서 작성자
문석환, 박상경, 박윤서, 서범창 
보고일
2025.12.20.
관련 문서
https://docs.zeek.org/en/master/index.html

2. 프로젝트 목적
본 프로젝트는 네트워크 통신 과정에서 발생할 수 있는 보안 위협을 선제적으로 차단하고, 내부 중요 자산의 유출을 방지하는 것을 목표로 함. 이를 위해 특정 네트워크 인터페이스(NIC)를 경유하는 트래픽을 실시간으로 수집, 분석하여 다음과 같은 보안 체계를 구축.
- 지능형 위협 탐지(Inbound Security): 머신러닝(ML) 알고리즘을 적용하여 내부 서버로 유입되는 트래픽의 정상/악성 여부를 고도화된 방식으로 판별.
- 정보 유출 방지(Outbound Security): 외부로 송출되는 데이터에 DLP(Data Loss Prevention) 솔루션을 적용하여 개인정보 및 기밀 데이터의 무단 유출을 원천 차단.
3. 핵심 요약
프로젝트의 전반적인 진행 상황에 대한 간략한 개요.

현재 상태: 완료
주요 성과: 
ML 기반 TLS 1.3 암호화 데이터 악성여부 판단
DLP 솔루션 적용
결론 및 권고 사항: 굿~
4. 시스템 아키텍처 및 적용 기술


5. 프로젝트 결과
1) ML 기반 악성/정상 트래픽 구분

zeek와 tcpdump를 이용하여 NIC을 통해 들어오는 트래픽을 캡처하고 분석함. cron 스케줄러를 통해 수집한 pcap 파일을 AutoEncoder의 입력으로 사용할 수 있도록 여러 feature를 선별하여 csv 파일로 만듦. 이렇게 생성한 csv 파일은 kafka로 전송.

CipherSpectrum 데이터셋을 활용하여 AutoEncoder 학습을 진행함. 데이터셋은 TLS 1.3 (AES128, AES256, chacha20) 으로 암호화된 flow 단위의 네트워크 트래픽 pcap으로 구성되어 있었고, 각 파일들을 Fwd/Bwd/Flow 단위로 세분화하여 각 feature들의 평균, 최소, 최대값을 추출하여 학습을 진행함. 악성 데이터셋은 https://www.malware-traffic-analysis.net/ 에서 최근 5개의 Exercise 데이터를 가져와 CipherSpectrum과 마찬가지로 feature들을 추출하여 평가 과정에서만 사용함.

학습 결과는 정상 데이터셋에 대해 0.96/0.97(Precision/Recall), 악성 데이터셋에 대해 0.97/0.80(Precision/Recall)으로 측정됨. 이후 AutoEncoder 복원 오차를 활용한 임계값 설정을 통해 정상과 악성 트래픽을 구분하는 기준치를 잡아보려고 했으나 실패. 

이후 복원 오차가 임계값을 넘기면 SHAP를 활용하여 어떤 feature가 악성으로 판단하도록 한 주 원인인지 파악 후 시각화. streamlit을 활용하여 관리자 대시보드를 만듦, 이 대시보드에서는 악성으로 판단된 flow와 주요하게 작용한 feature를 출력하고, TOP-3 feature를 강조.

예상 실패 요인
l. 정규화
정상 데이터셋과 악성 데이터셋의 볼륨이 다르고, 정규화를 거치지 않은 상태에서 학습과 평가를 진행했기에 오류가 생겼을 것이라 예상.

m. 악성 트래픽 데이터셋의 오류
CipherSpectrum과 다른 방법으로 악성 트래픽 데이터를 가져왔기 때문에 이 둘의 괴리로 인한 오류. 해당 원인을 해결하기는 어려울 것이라고 예상. 학습 완료 후에 나왔던 테스트 수치에 따르면, 정상 데이터에 관해서는 굉장히 높은 정확도로 걸러내고 있음을 확인했기에 정상 트래픽을 위주로 모델링을 다시 진행하는 것이 적절한 해결방법.

n. 실제 데이터와 데이터셋의 괴리
학습에 사용된 CipherSpectrum과 실제 서버로 들어오는 트래픽 데이터의 괴리로 인한 오작동. 이는 당연하게도, 해당 모델을 사용하고자 하는 환경에 맞는 데이터셋이 준비되지 않았기 때문에 발생한 문제. 프로젝트 한계상 미리 준비되어있는 데이터셋을 활용하고자 했으나 실제로 이 프로젝트를 적용하고자 한다면 데이터 수집과 전처리를 직접 진행해야 해결 가능.

o. 하이퍼파라미터 튜닝 실패
학습률, 손실 함수, epoch, 레이어 개수 등 적절한 파라미터의 값을 찾지 못한 것이 원인. 하이퍼 파라미터 튜닝 방법론을 더 학습한 후, 가장 적합한 파라미터 값을 찾아낸다면 정확도가 소폭 상승할 것으로 예상.


2) DLP 솔루션 적용

mitmproxy를 기반으로 HTTP/HTTPS 트래픽을 복호화한 후, 요청(Request) 및 응답(Response)에 대해 Rule-based 분석과 AI-based 분석을 수행
HTTPS 트래픽은 중간자(MITM) 방식으로 복호화되며, 탐지 정책에 위반되는 트래픽에 대해서는 즉시 403 Forbidden 응답을 반환하여 데이터 전송을 차단하고, 동시에 로그 파일에 탐지 사유를 기록.
	
2-1) Rule-based 
사전에 정의된 정책과 패턴을 기반으로 한 정적 분석 방식으로, 오탐을 줄이기 위해 Allowlist → 탐지 → 추가 조건 검사의 단계적 구조로 구성
2.1.1 Allowlist 기반 예외 처리
사전에 허용된 도메인은 검사 대상에서 제외
불필요한 내부 트래픽 및 정상 서비스 요청에 대한 오탐 최소화
2.1.2 정규식 기반 민감 정보 탐지
주민등록번호, 카드번호, 은행 계좌번호 등 정형화된 민감 정보를 정규표현식을 통해 탐지
날짜 유효성 검증을 포함하여 단순 숫자 패턴에 의한 오탐을 최소화
카드사 이름, 은행 이름 등을 추가 조사하여 오탐 최소화
2.1.3 키워드 기반 탐지
password, secret, internal-only 등의 민감 키워드 출현 빈도를 기준으로 탐지
특정 빈도에서는 허용된 도메인 또는 페이로드 크기 검사 등의 추가 조사
2.1.4 GitHub 업로드 탐지
소스 코드 유출을 방지하기 위해 GitHub 웹 UI 파일 업로드 행위를 탐지
REST API를 통한 GitHub 업로드 행위 탐지
로컬에서 git push 행위 탐지 

2-2) Artificial Intelligence
오픈소스로 공개된 Presidio를 이용한 특정 정보 탐색

2.2.1 Presidio
Presidio란 MicroSoft에서 개발한 오픈소스 탐색 도구로, 주민번호, 주소 등의 민감 정보가 텍스트에서 감지된다면 이를 탐색 후 마스킹이 가능하다.
2.2.2 프로젝트 적용
본 프로젝트에선 마스킹이 필요하지 않으므로, Presidio의 분석 및 탐색 기능만 사용을 하였으며 Request 및 Response 패킷의 내용을 탐지하여 해당 Payload 텍스트를 Presidio에 입력값으로 넣은 다음, 만약 민감 정보가 감지될 경우 해당 패킷을 차단시키는 방식을 사용하였다.
2.2.3 개선 요인
현재 Presidio의 Parser 모델 중 한국어를 파싱하는 정상적으로 작동하는 분석 모델을 찾지 못하여 원문이 영어로 되어 있는 경우만 탐지가 가능한 상태이며, 이를 위해선 한국어 텍스트를 정확히 파싱하는 모델을 학습시켜 적용할 필요가 있다고 생각한다. 

Rule-based : 허용된 리스트 기반으로 검사 제외 후 요청에 대해서는 정규식을 통해 주민 번호와 카드/은행 번호와 같은 민감 정보 검사, 키워드 기반 검사, 깃허브 업로드 검사 진행. 응답에 대해서는 민감 정보 검사만 진행 
AI : Presidio 오픈소스를 이용한 특정 정보 탐색 및 접속 신호 차단, 다만 한국어 내용은 탐지하는 모델을 찾을 수 없어 영어로 이루어진 원문 내용만 탐지 가능한 상태 
