---
title: "[2025-12-22] 프로젝트 보고서"
excerpt: "설명충 프로젝트"

categories:
  - Project
tags:
  - [NetWork]

permalink: /Project/[2025-12-22] 프로젝트 보고서/

toc: true
toc_sticky: true

date: 2025-12-22
last_modified_at: 2025-12-22
---

프로젝트 보고서
1. 프로젝트 개요

항목
내용
프로젝트명
Explain Wall
보고서 작성자
문석환, 박상경, 박윤서, 서범창 
보고일
2025.12.20.
관련 문서
https://docs.zeek.org/en/master/index.html

2. 프로젝트 목적
본 프로젝트는 네트워크 통신 과정에서 발생할 수 있는 보안 위협을 선제적으로 차단하고, 내부 중요 자산의 유출을 방지하는 것을 목표로 함. 이를 위해 특정 네트워크 인터페이스(NIC)를 경유하는 트래픽을 실시간으로 수집, 분석하여 다음과 같은 보안 체계를 구축.
- 지능형 위협 탐지(Inbound Security): 머신러닝(ML) 알고리즘을 적용하여 내부 서버로 유입되는 트래픽의 정상/악성 여부를 고도화된 방식으로 판별.
- 정보 유출 방지(Outbound Security): 외부로 송출되는 데이터에 DLP(Data Loss Prevention) 솔루션을 적용하여 개인정보 및 기밀 데이터의 무단 유출을 원천 차단.
3. 핵심 요약
프로젝트의 전반적인 진행 상황에 대한 간략한 개요.

현재 상태: 완료
주요 성과: 
ML 기반 TLS 1.3 암호화 데이터 악성여부 판단
DLP 솔루션 적용
결론 및 권고 사항: 굿~
4. 시스템 아키텍처 및 적용 기술
![image.png](../assets/images/posts_img/2025-12-22%20보고서/image.png)
# 5. 문제

Kafka에서 문제점 : 공식 문서가 아닌 곳에서 봐서 그룹이름 지정 안해서 이상하게 읽음

dlp : 공식 문서 봤는 데 자꾸 오탐이 발생. → mitmproxy 문서만 봤기 때문. rule-based dlp를 하는 여러 회사의 문서를  참조하게 됨. 

mitmproxy 할 때 로컬에서는 잘됐는 데 서버 컴퓨터에서는 안됨 → 크롬이 아닌 파이어폭스이라 달랐음 

kafka → autoencoder 연결할 때 github는 잘 올려놔서 괜찮았는 데 환경 변수 처리한 부분 때문에 곤란했음.   

### ① Kafka Consumer Group 설정 부재로 인한 데이터 정합성 오류

- **현상:** Kafka를 통해 데이터를 소비하는 과정에서 메시지가 중복으로 읽히거나, 읽어야 할 데이터를 놓치는 등 이상 현상 발생.
- **원인:** 공식 문서가 아닌 블로그 등의 레퍼런스를 참고하면서 `group.id` 설정의 중요성을 간과함. Consumer Group이 명시되지 않아 매번 새로운 그룹으로 인식되거나 오프셋(Offset) 관리가 정상적으로 이루어지지 않음.

### ② DLP(Data Loss Prevention) 오탐 발생 및 탐지 규칙 한계

- **현상:** 특정 패턴의 데이터가 유출되지 않았음에도 불구하고 지속적으로 차단 알림이 발생하는 오탐(False Positive) 현상 반복.
- **원인:** `mitmproxy` 라이브러리 자체 문서에만 의존하여 탐지 로직을 구현함. 실제 네트워크 환경의 복잡한 데이터 패턴을 반영하지 못한 단순 규칙 기반 탐지의 한계 확인.

### ③ 환경 차이(Local vs Server)로 인한 mitmproxy 동작 오류

- **현상:** 로컬 개발 환경에서는 정상 작동하던 프록시 기능이 서버 배포 후 동작하지 않음.
- **원인:** 브라우저 엔진 차이로 인한 인증서 및 트래픽 처리 방식의 상이함 확인. 로컬은 **Chrome**, 서버는 **Firefox** 환경이었으며, 각 브라우저가 SSL/TLS 트래픽을 처리하고 프록시를 설정하는 세부 방식이 달랐음.

### ④ 배포 환경에서의 환경 변수(Environment Variable) 충돌

- **현상:** Kafka와 Autoencoder 모델 서버를 연결하는 과정에서 코드상에는 문제가 없으나, 서버 실행 시 연결 오류 발생.
- **원인:** GitHub에 업로드된 소스 코드는 정상적이었으나, 보안을 위해 분리한 **환경 변수(.env)** 설정이 서버 환경과 일치하지 않거나 누락되어 데이터 파이프라인 연결 실패.

---

## 6. 해결 방안 및 성과

### ① 공식 문서 기반의 설정 표준화 (Kafka)

- **조치:** 외부 블로그 대신 **Apache Kafka 공식 가이드라인**을 전수 조사하여 설정을 재검토함.
- **결과:** 서비스 목적에 맞는 `group.id`를 명확히 부여하고, Offset 관리 전략을 수립하여 데이터 소비의 **일관성(Consistency)**과 **안정성**을 확보함.

### ② 다각도 벤치마킹을 통한 탐지 로직 고도화 (DLP)

- **조치:** 단일 라이브러리 문서에서 벗어나, Rule-based DLP를 서비스하는 **보안 전문 기업들의 기술 블로그와 화이트 페이퍼**를 참조하여 정규표현식 및 탐지 알고리즘을 개선함.
- **결과:** 예외 처리 규칙을 세분화하여 **오탐률을 기존 대비 00% 감소**시켰으며, 보다 정교한 필터링 체계 구축.

### ③ 크로스 브라우저 대응 및 환경 격차 해소

- **조치:** 서버 환경의 Firefox 설정에 맞게 `mitmproxy` 인증서 신뢰 설정을 수동으로 추가하고, 브라우저별 네트워크 설정 스크립트를 표준화함.
- **결과:** 실행 환경에 구애받지 않는 **프록시 서버 안정화** 달성 및 환경 차이로 인한 디버깅 시간 단축.

### ④ 환경 변수 관리 체계 구축 및 배포 자동화 점검

- **조치:** 하드코딩된 설정을 완전히 배제하고, 서버별 환경 변수 체크리스트를 작성함. `python-dotenv` 등을 활용해 로컬과 서버의 설정 파일을 엄격히 분리 관리함.
- **결과:** 환경 설정 오류로 인한 장애를 사전에 방지하고, **애플리케이션의 이식성(Portability)** 향상.


5. 프로젝트 결과
1) ML 기반 악성/정상 트래픽 구분

zeek와 tcpdump를 이용하여 NIC을 통해 들어오는 트래픽을 캡처하고 분석함. cron 스케줄러를 통해 수집한 pcap 파일을 AutoEncoder의 입력으로 사용할 수 있도록 여러 feature를 선별하여 csv 파일로 만듦. 이렇게 생성한 csv 파일은 kafka로 전송.

CipherSpectrum 데이터셋을 활용하여 AutoEncoder 학습을 진행함. 데이터셋은 TLS 1.3 (AES128, AES256, chacha20) 으로 암호화된 flow 단위의 네트워크 트래픽 pcap으로 구성되어 있었고, 각 파일들을 Fwd/Bwd/Flow 단위로 세분화하여 각 feature들의 평균, 최소, 최대값을 추출하여 학습을 진행함. 악성 데이터셋은 https://www.malware-traffic-analysis.net/ 에서 최근 5개의 Exercise 데이터를 가져와 CipherSpectrum과 마찬가지로 feature들을 추출하여 평가 과정에서만 사용함.

학습 결과는 정상 데이터셋에 대해 0.96/0.97(Precision/Recall), 악성 데이터셋에 대해 0.97/0.80(Precision/Recall)으로 측정됨. 이후 AutoEncoder 복원 오차를 활용한 임계값 설정을 통해 정상과 악성 트래픽을 구분하는 기준치를 잡아보려고 했으나 실패. 

이후 복원 오차가 임계값을 넘기면 SHAP를 활용하여 어떤 feature가 악성으로 판단하도록 한 주 원인인지 파악 후 시각화. streamlit을 활용하여 관리자 대시보드를 만듦, 이 대시보드에서는 악성으로 판단된 flow와 주요하게 작용한 feature를 출력하고, TOP-3 feature를 강조.

예상 실패 요인
l. 정규화
정상 데이터셋과 악성 데이터셋의 볼륨이 다르고, 정규화를 거치지 않은 상태에서 학습과 평가를 진행했기에 오류가 생겼을 것이라 예상.

m. 악성 트래픽 데이터셋의 오류
CipherSpectrum과 다른 방법으로 악성 트래픽 데이터를 가져왔기 때문에 이 둘의 괴리로 인한 오류. 해당 원인을 해결하기는 어려울 것이라고 예상. 학습 완료 후에 나왔던 테스트 수치에 따르면, 정상 데이터에 관해서는 굉장히 높은 정확도로 걸러내고 있음을 확인했기에 정상 트래픽을 위주로 모델링을 다시 진행하는 것이 적절한 해결방법.

n. 실제 데이터와 데이터셋의 괴리
학습에 사용된 CipherSpectrum과 실제 서버로 들어오는 트래픽 데이터의 괴리로 인한 오작동. 이는 당연하게도, 해당 모델을 사용하고자 하는 환경에 맞는 데이터셋이 준비되지 않았기 때문에 발생한 문제. 프로젝트 한계상 미리 준비되어있는 데이터셋을 활용하고자 했으나 실제로 이 프로젝트를 적용하고자 한다면 데이터 수집과 전처리를 직접 진행해야 해결 가능.

o. 하이퍼파라미터 튜닝 실패
학습률, 손실 함수, epoch, 레이어 개수 등 적절한 파라미터의 값을 찾지 못한 것이 원인. 하이퍼 파라미터 튜닝 방법론을 더 학습한 후, 가장 적합한 파라미터 값을 찾아낸다면 정확도가 소폭 상승할 것으로 예상.


2) DLP 솔루션 적용

mitmproxy를 기반으로 HTTP/HTTPS 트래픽을 복호화한 후, 요청(Request) 및 응답(Response)에 대해 Rule-based 분석과 AI-based 분석을 수행
HTTPS 트래픽은 중간자(MITM) 방식으로 복호화되며, 탐지 정책에 위반되는 트래픽에 대해서는 즉시 403 Forbidden 응답을 반환하여 데이터 전송을 차단하고, 동시에 로그 파일에 탐지 사유를 기록.
	
2-1) Rule-based 
사전에 정의된 정책과 패턴을 기반으로 한 정적 분석 방식으로, 오탐을 줄이기 위해 Allowlist → 탐지 → 추가 조건 검사의 단계적 구조로 구성
2.1.1 Allowlist 기반 예외 처리
사전에 허용된 도메인은 검사 대상에서 제외
불필요한 내부 트래픽 및 정상 서비스 요청에 대한 오탐 최소화
2.1.2 정규식 기반 민감 정보 탐지
주민등록번호, 카드번호, 은행 계좌번호 등 정형화된 민감 정보를 정규표현식을 통해 탐지
날짜 유효성 검증을 포함하여 단순 숫자 패턴에 의한 오탐을 최소화
카드사 이름, 은행 이름 등을 추가 조사하여 오탐 최소화
2.1.3 키워드 기반 탐지
password, secret, internal-only 등의 민감 키워드 출현 빈도를 기준으로 탐지
특정 빈도에서는 허용된 도메인 또는 페이로드 크기 검사 등의 추가 조사
2.1.4 GitHub 업로드 탐지
소스 코드 유출을 방지하기 위해 GitHub 웹 UI 파일 업로드 행위를 탐지
REST API를 통한 GitHub 업로드 행위 탐지
로컬에서 git push 행위 탐지 

2-2) Artificial Intelligence
오픈소스로 공개된 Presidio를 이용한 특정 정보 탐색

2.2.1 Presidio
Presidio란 MicroSoft에서 개발한 오픈소스 탐색 도구로, 주민번호, 주소 등의 민감 정보가 텍스트에서 감지된다면 이를 탐색 후 마스킹이 가능하다.
2.2.2 프로젝트 적용
본 프로젝트에선 마스킹이 필요하지 않으므로, Presidio의 분석 및 탐색 기능만 사용을 하였으며 Request 및 Response 패킷의 내용을 탐지하여 해당 Payload 텍스트를 Presidio에 입력값으로 넣은 다음, 만약 민감 정보가 감지될 경우 해당 패킷을 차단시키는 방식을 사용하였다.
2.2.3 개선 요인
현재 Presidio의 Parser 모델 중 한국어를 파싱하는 정상적으로 작동하는 분석 모델을 찾지 못하여 원문이 영어로 되어 있는 경우만 탐지가 가능한 상태이며, 이를 위해선 한국어 텍스트를 정확히 파싱하는 모델을 학습시켜 적용할 필요가 있다고 생각한다. 

Rule-based : 허용된 리스트 기반으로 검사 제외 후 요청에 대해서는 정규식을 통해 주민 번호와 카드/은행 번호와 같은 민감 정보 검사, 키워드 기반 검사, 깃허브 업로드 검사 진행. 응답에 대해서는 민감 정보 검사만 진행 
AI : Presidio 오픈소스를 이용한 특정 정보 탐색 및 접속 신호 차단, 다만 한국어 내용은 탐지하는 모델을 찾을 수 없어 영어로 이루어진 원문 내용만 탐지 가능한 상태 
