---
title: "[2025-12-22] 프로젝트 보고서"
excerpt: "설명충 프로젝트"

categories:
  - Project
tags:
  - [NetWork]

permalink: /Project/[2025-12-22] 프로젝트 보고서/

toc: true
toc_sticky: true

date: 2025-12-22
last_modified_at: 2025-12-22
---

# **1. 프로젝트 개요**

| 항목 | 내용 |
| --- | --- |
| 프로젝트명 | Explain Wall |
| 보고서 작성자 | 문석환, 박상경, 박윤서, 서범창 |
| 보고일 | 2025.12.20 |
| 관련 문서 | https://docs.zeek.org/en/master/index.html |

---

# **2. 프로젝트 목적**

본 프로젝트는 네트워크 통신 과정에서 발생할 수 있는 보안 위협을 선제적으로 탐지·차단하고, 내부 중요 자산의 외부 유출을 방지하는 것을 목표로 함.

이를 위해 특정 네트워크 인터페이스(NIC)를 경유하는 트래픽을 실시간으로 수집·분석하여 다음과 같은 보안 체계를 구축.

- **지능형 위협 탐지 (Inbound Security)**
    
    머신러닝(ML) 기반 분석을 통해 내부 서버로 유입되는 암호화 트래픽의 정상/악성 여부를 판단
    
- **정보 유출 방지 (Outbound Security)**
    
    외부로 송출되는 데이터에 DLP(Data Loss Prevention) 솔루션을 적용하여 개인정보 및 기밀 데이터 유출 차단
    

---

# **3. 핵심 요약**

프로젝트의 전반적인 진행 상황 및 성과는 다음과 같음.

- **현재 상태:** 완료
- **주요 성과**
    - TLS 1.3 암호화 트래픽에 대한 ML 기반 악성 여부 판단 가능성 검증
        - 공개 데이터셋에서는 유효하나, 실제 수집 데이터에 대해서는 일반화 한계 확인
    - Rule-based + AI 기반 DLP 솔루션 적용
- **결론:** 프로젝트 목표를 충족하였으며, 실제 환경 적용 가능성을 확인함

---

# **4. 시스템 아키텍처 및 적용 기술**

![image.png](https://yunseo10987.github.io/assets/images/posts_img/2025-12-22%20보고서/image.png)

- `Zeek` 및 tcpdump를 활용한 네트워크 트래픽 수집
- `Kafka` 기반 실시간 데이터 스트리밍
    - Kafka Cluster 및 Zookeeper를 `Docker` 컨테이너 환경에서 구성하여 확장성과 관리 용이성 확보
- AutoEncoder 기반 이상 탐지 모델
    - 각 AutoEncoder 모델을 `Docker-in-Docker` 환경에서 컨테이너 단위로 분리 배포하여 병렬 처리 및 확장성을 확보
- `SHAP` 기반 이상 탐지 결과 설명(XAI)
- `mitmproxy` 기반 DLP 솔루션 적용
- `Presidio`를 통한 AI 기반으로 민감 정보 감지 후 마스킹

---

# **5. 문제점 및 원인 분석**

- Kafka Consumer Group 설정 부재로 인한 데이터 정합성 문제
    - **현상** : Kafka Consumer에서 메시지가 중복 수신되거나 일부 메시지 누락
    - **원인** : 공식 문서가 아닌 외부 블로그를 참고하여 개발. Consumer Group에 대한 중요성을 인지하지 못하여 Offset 관리가 정상적으로 이루어지지 않음
- DLP 오탐(False Positive) 발생
    - **현상** : 실제 민감 정보가 포함되지 않은 트래픽임에도 불구하고 반복적인 차단 알림 발생
    - **원인** : `mitmproxy` 공식 문서만을 참고하여 단순 규칙 기반 탐지 로직을 구현하여 실제 서비스 환경의 복잡한 패턴을 반영하지 못함
- Local과 Server의 환경 차이로 인한 `mitmproxy` 동작 오류
    - **현상** : 로컬 환경에서는 정상 작동하던 `mitmproxy`가 서버 환경에서는 정상 동작하지 않음
    - **원인** : 로컬은 **Chrome**, 서버는 **Firefox** 환경으로 브라우저별 SSL/TLS 처리 및 인증서 신뢰 방식 차이 존재
- 환경 변수 설정 오류로 인한 서비스 연동 실패
    - **현상** : Kafka와 Autoencoder 모델 서버를 연결하는 과정에서 코드상에는 문제가 없으나, 서버 실행 시 연결 오류 발생.
    - **원인** : GitHub에 업로드된 소스 코드는 정상적이었으나, 보안을 위해 분리한 `.env` 파일 설정이 서버 환경과 일치하지 않거나 누락되어 데이터 파이프라인 연결 실패.

---

# **6. 해결 방안 및 성과**

- `Kafka` 설정 표준화
    - **조치** : 외부 블로그 대신 Apache Kafka 공식 문서를 기반으로 Consumer Group 및 Offset 관리 정책 설정 재검토
    - **결과** : 데이터 소비의 일관성(Consistency) 및 안정성 확보
- DLP 탐지 로직 고도화
    - **조치** : 보안 전문 기업의 기술 블로그 및 화이트페이퍼를 참고하여 Rule-based 탐지 규칙 개선
    - **결과** : 예외 처리 및 조건 분기 세분화하여 오탐률 감소
- 크로스 브라우저 환경 대응
    - **조치** : Firefox 환경에 맞는 mitmproxy 인증서 수동 등록
    - **결과** : 프록시 서버 안정화 달성
- 환경 변수 관리 체계 정립
    - **조치** : 환경 변수 및 실행 설정을 Notion 기반 문서로 정리하여 전달
    - **결과** : 환경 변수 누락이나 배포 오류감소

---

# **7. 프로젝트 결과**

## **1) ML 기반 악성/정상 트래픽 분류**

Zeek와 tcpdump를 이용하여 NIC을 통해 유입되는 트래픽을 수집하고, cron 스케줄러를 활용하여 pcap 파일을 주기적으로 생성.

수집된 트래픽은 AutoEncoder 모델 입력을 위해 여러 feature를 선별하여 CSV 형태로 가공한 후 Kafka로 전송하였다.

- **학습 데이터셋**
    - **정상 데이터:** CipherSpectrum (TLS 1.3, AES128/AES256/ChaCha20)
    - **악성 데이터:** malware-traffic-analysis.net 최근 5개의 Exercise 데이터
    
    각 flow에 대해 Fwd/Bwd/Flow 단위 feature의 평균·최소·최대값을 추출하여 학습에 사용.
    
- **성능 평가**
    - **정상 트래픽**: **Precision 0.96 / Recall 0.97**
    - **악성 트래픽**: **Precision 0.97 / Recall 0.80**
    
    AutoEncoder 복원 오차 기반 임계값 설정은 실패하였으나, SHAP를 활용하여 악성 판단에 기여한 주요 feature를 시각화하였다.
    
    이를 Streamlit 기반 관리자 대시보드로 구현하여, 악성 flow 및 TOP-3 주요 feature를 직관적으로 확인 가능하도록 구성하였다.
    
- **예상 실패 요인**
    - **정규화 미적용**
        
        정상 데이터와 악성 데이터셋의 볼륨이 다르고, 정규화를 거치지 않은 상태에서 학습과 평가 진행
        
    - **악성 데이터셋 수집 방식 차이**
        
        CipherSpectrum과는 다른 수집 방식으로 악성 트래픽 데이터를 사용하여 정상과 악성 데이터셋 간의 데이터셋 간 데이터 불일치. 평가 결과 정상 트래픽에 대해서는 높은 정확도로 이상 여부를 구분하고 있음을 확인하였으며, 이를 바탕으로 정상 트래픽 중심의 모델링 및 재학습이 개선 방향으로 예상 
        
    - **실제 트래픽과 데이터셋 간 괴리**
        
        학습에 사용된 CipherSpectrum과 실제 서버로 들어오는 트래픽 데이터의 차이점. 모델 적용 환경에 적합한 데이터셋이 충분히 확보되지 않은 한계로, 실제 서비스 적용을 위해서는 환경에 맞는 트래픽 수집 및 전처리 과정이 필수임을 확인 
        
    - **하이퍼파라미터 튜닝 부족**
        
        학습률, 손실 함수, epoch, 레이어 개수 등의 하이퍼파라미터에 대해 충분한 조정이 이루어지지 못하여 최적의 설정을 탐색하지 못함. 하이퍼 파라미터 튜닝 방법론을 더 학습한 후, 가장 적합한 파라미터 값을 찾아낸다면 모델 성능이 향상될 것으로 예상.
        

---

## **2) DLP 솔루션 적용**

mitmproxy를 기반으로 HTTP/HTTPS 트래픽을 MITM 방식으로 복호화한 후, 요청(Request)과 응답(Response)에 대해 분석을 수행.

정책 위반 트래픽은 즉시 403 Forbidden 응답으로 차단하고, 탐지 사유를 로그로 기록.

### 2-1) Rule-based 분석

- **Allowlist 기반 예외 처리**
    
    오탐을 줄이기 위해 사전에 허용된 도메인은 검사 대상에서 제외
    
- **정규표현식 기반 민감 정보 탐지**
    
    주민등록번호, 카드 번호, 은행 계좌번호 등 정형화된 민감 정보를 정규 표현식을 통해 탐지. 주민등록번호의 경우에는 날짜 유효성을 검증을 포함하고, 카드 번호, 계좌 번호의 같은 경우에는 카드사 이름과 은행 이름 탐지를 포함하여 오탐을 최소화
    
- **키워드 기반 탐지**
    
    Password, secret 등의 민감 키워드 출현 빈도를 기준으로 탐지하고 특정 빈도의 경우에는 도메인과 페이로드 크기 등의 추가적인 조사 진행. 
    
- **GitHub 웹 UI / REST API / git push 업로드 탐지**
    
    소스 코드 유출을 방지하기 위해 Github 웹 UI, REST API,  git push 행위 탐지 
    

### 2-2) AI 기반 분석

- Microsoft Presidio를 활용한 민감 정보 탐지
    
    MicroSoft에서 개발한 오픈소스 탐색 도구 `Presidio`를 활용하여 텍스트 탐지 후 마스킹. 현재는 영어만 탐지 가능하여 한국어 탐지를 위해 추가 모델 학습이 필요 
    

---

## **8. 결론**

ML 기반 이상 탐지는 공개 데이터셋 환경에서는 유효성을 확인하였으나, 실제 운영 환경에 적용하기 위해서는 환경 맞춤형 데이터 수집 및 재학습이 필수적임을 확인.

이를 통해 본 프로젝트는 모델 성능 검증뿐 아니라, 실제 네트워크 보안 시스템 구축 시 데이터 일반화의 한계를 실증적으로 확인한 사례로서 의미를 가짐.

한편, DLP 솔루션은 실환경 트래픽을 대상으로 안정적인 차단 동작을 수행하였으며, Rule-based 정책과 AI 기반 분석을 결합함으로써 정보 유출 방지 측면에서 실용적인 적용 가능성을 입증.